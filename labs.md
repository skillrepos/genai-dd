# Generative AI for Developers Deep Dive
## Understanding key Gen AI concepts - full-day workshop
## Session labs 
## Revision 5.6 - 07/12/25

**Follow the startup instructions in the README.md file IF NOT ALREADY DONE!**

**NOTE: To copy and paste in the codespace, you may need to use keyboard commands - CTRL-C and CTRL-V. Chrome may work best for this.**

**Lab 1 - Working with Neural Networks**

**Purpose: In this lab, we’ll learn more about neural networks by seeing how one is coded and trained.**

1. In our repository, we have a set of Python programs to help us illustrate and work with concepts in the labs. These are mostly in the *genai* subdirectory. Go to the *TERMINAL* tab in the bottom part of your codespace and change into that directory.
```
cd genai
```

2. For this lab, we have a simple neural net coded in Python. The file name is nn.py. Open the file either by clicking on [**genai/nn.py**](./genai/nn.py) or by entering the command below in the codespace's terminal.

```
code nn.py
```

3. Scroll down to around line 55. Notice the *training_inputs* data and the *training_outputs* data. Each row of the *training_outputs* is what we want the model to predict for the corresponding input row. As coded, the output for the sample inputs ends up being the same as the first element of the array.  For inputs [0,0,1] we are trying to train the model to predict [0]. For the inputs [1,0,1], we are trying to train the model to predict [1], etc. The table below may help to explain.

| **Dataset** | **Values** | **Desired Prediction** |
| :---------: | :--------: | :--------------------: |
| **1** |  0  0  1  |            0           |
| **2** |  1  1  1  |            1           |
| **3** |  1  0  1  |            1           |
| **4** |  0  1  1  |            0           |

4. When we run the program, it will train the neural net to try and predict the outputs corresponding to the inputs. You will see the random training weights to start and then the adjusted weights to make the model predict the output. You will then be prompted to put in your own training data. We'll look at that in the next step. For now, go ahead and run the program (command below) but don't put in any inputs yet. Just notice how the weights have been adjusted after the training process.

```
python nn.py
```
![Starting run of simple nn](./images/gaidd30.png?raw=true "Starting run of simple nn") 

5. What you should see is that the weights after training are now set in a way that makes it more likely that the result will match the expected output value. (The higher positive value for the first weight means that the model has looked at the training data and realized it should "weigh" the first input higher in its prediction.) To prove this out, you can enter your own input set - just use 1's and 0's for each input. 

![Inputs to simple nn](./images/gaidd31.png?raw=true "Inputs to simple nn") 

6. After you put in your inputs, the neural net will process your input and because of the training, it should predict a result that is close to the first input value you entered (the one for *Input one*).

![Prediction close to first input](./images/gaidd32.png?raw=true "Prediction close to first input") 

7. Now, let's see what happens if we change the expected outputs to be different. In the editor for the genai_nn.py file, find the line for the *training_outputs*. Modify the values in the array to be ([[0],[1],[0],[1]]). These are the values of the second element in each of the training data entries. After you're done, save your changes as shown below, or use the keyboard shortcut.

![Modifying expected outputs](./images/gaidd33.png?raw=true "Modifying expected outputs")
![Saving changes](./images/gaidd9.png?raw=true "Saving changes")

8. Now, run the neural net again. This time when the weights after training are shown, you should see a bias for a higher weight for the second item.
```
python nn.py
```
![Second run of simple nn](./images/gaidd34.png?raw=true "Second run of simple nn") 

9. At the input prompts, just input any sequence of 0's and 1's as before.

10. When the trained model then processes your inputs, you should see that it predicts a value that is close to 0 or 1 depending on what your second input was.

![Second output of simple nn](./images/gaidd35.png?raw=true "Second output of simple nn")

11. (Optional) If you get done early and want more to do, feel free to try other combinations of training inputs and training outputs.
    
<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 2 - Experimenting with Tokenization**

**Purpose: In this lab, we'll see how different models do tokenization.**

1. In the same *genai* directory, we have a simple program that can load a model and print out tokens generated by it. The file name is *tokenizer.py*. You can view the file either by clicking on [**genai/tokenizer.py**](./genai/tokenizer.py) or by entering the command below in the codespace's terminal (assuming you're still in the *genai* directory).

```
code tokenizer.py
```
2. This program can be run and passed a model to use for tokenization. To start, we'll be using a model named *bert-base-uncased*. Let's look at this model on huggingface.co.  Go to https://huggingface.co/models and in the *Models* search area, type in *bert-base-uncased*. Select the entry for *google-bert/bert-base-uncased*.

![Finding bert model on huggingface](./images/gaidd12.png?raw=true "Finding bert model on huggingface")

3. Once you click on the selection, you'll be on the *model card* tab for the model. Take a look at the model card for the model and then click on the *Files and Versions* and *Community* tabs to look at those pages.

![huggingface tabs](./images/gaidd13.png?raw=true "huggingface tabs")

4. Now let's switch back to the codespace and, in the terminal, run the *tokenizer* program with the *bert-base-uncased* model. Enter the command below. This will download some of the files you saw on the *Files* tab for the model in HuggingFace.
```
python tokenizer.py bert-base-uncased
```
5. After the program starts, you will be at a prompt to *Enter text*. Enter in some text like the following to see how it will be tokenized.
```
This is sample text for tokenization and text for embeddings.
```
![input for tokenization](./images/gaidd36.png?raw=true "input for tokenization")

6. After you enter this, you'll see the various subword tokens that were extracted from the text you entered. And you'll also see the ids for the tokens stored in the model that matched the subwords.

![tokenization output](./images/gaidd37.png?raw=true "tokenization output")

7. Next, you can try out some other models. Repeat steps 4 - 6 for other tokenizers like the following. (You can use the same text string or different ones. Notice how the text is broken down depending on the model and also the meta-characters.)
```
python tokenizer.py roberta-base
python tokenizer.py gpt2
python tokenizer.py xlnet-large-cased
```
8. (Optional) If you finish early and want more to do, you can look up the models from step 7 on huggingface.co/models.
   
<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 3 - Understanding embeddings, vectors and similarity measures**

**Purpose: In this lab, we'll see how tokens get mapped to vectors and how vectors can be compared.**

1. In the repository, we have a Python program that uses a Tokenizer and Model to create embeddings for three terms that you input. It then computes and displays the cosine similarity between each combination. Open the file to look at it by clicking on [**genai/vectors.py**](./genai/vectors.py) or by using the command below in the terminal.
```
code vectors.py
```
2. Let's run the program. As we did for the tokenizer example, we'll pass in a model to use. We'll also pass in a second argument which is the number of dimensions from the vector for each term to show. Run the program with the command below. You can wait to enter terms until the next step.
```
python vectors.py bert-base-cased 5
```
![vectors program run](./images/gaidd38.png?raw=true "vectors program run")

3. The command we just ran loads up the bert-base-cased model and tells it to show the first 5 dimensions of each vector for the terms we enter. The program will be prompting you for three terms. Enter each one in turn. You can try two closely related words and one that is not closely related. For example
   - king
   - queen
   - duck

![vectors program inputs](./images/gaidd39.png?raw=true "vectors program inputs")

4. Once you enter the terms, you'll see the first 5 dimensions for each term. And then you'll see the cosine similarity displayed between each possible pair. This is how similar each pair of words is. The two that are most similar should have a higher cosine similarity "score".

![vectors program outputs](./images/gaidd40.png?raw=true "vectors program outputs")

5. Each vector in the bert-based models have 768 dimensions. Let's run the program again and tell it to display 768 dimensions for each of the three terms.  Also, you can try another set of terms that are more closely related, like *multiplication*, *division*, *addition*.
```
python vectors.py bert-base-cased 768
```
6. You should see that the cosine similarities for all pair combinations are not as far apart this time.
![vectors program second outputs](./images/gaidd19.png?raw=true "vectors program second outputs")

7. As part of the output from the program, you'll also see the *token id* for each term. (It is above the print of the dimensions. If you don't want to scroll through all the dimensions, you can just run it again with a small number of dimensions like we did in step 2.) If you're using the same model as you did in lab 2 for tokenization, the ids will be the same. 

![token id](./images/gaidd20.png?raw=true "token id")

8. You can actually see where these mappings are stored if you look at the model on Hugging Face. For instance, for the *bert-base-cased* model, you can go to https://huggingface.co and search for bert-base-cased. Select the entry for google-bert/bert-base-cased.

![finding model](./images/gaidd21.png?raw=true "finding model")

8. On the page for the model, click on the *Files and versions* tab. Then find the file *tokenizer.json* and click on it. The file will be too large to display, so click on the *check the raw version* link to see the actual content.

![selecting tokenizer.json](./images/gaidd22.png?raw=true "selecting tokenizer.json")
![opening file](./images/gaidd23.png?raw=true "opening file")

9. You can search for the terms you entered previously with a Ctrl-F or Cmd-F and find the mapping between the term and the id. If you look for "##" you'll see mappings for parts of tokens like you may have seen in lab 2.

![finding terms in file](./images/gaidd24.png?raw=true "finding terms in files")

10. If you want, you can try running the *genai_vectors.py* program with a different model to see results from other models (such as we used in lab 2) and words that are very close like *embeddings*, *tokenization*, *subwords*.

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 4 - Working with transformer models**

**Purpose: In this lab, we’ll see how to interact with various models for different standard tasks**

1. In our repository, we have several different Python programs that utilize transformer models for standard types of LLM tasks. One of them is a simple translation example. The file name is *translation.py*. Open the file either by clicking on [**genai/translation.py**](./genai/translation.py) or by entering the command below in the codespace's terminal.

```
code translation.py
```
2. Take a look at the file contents.  Notice that we are pulling in a specific model ending with 'en-fr'. This is a clue that this model is trained for English to French translation. Let's find out more about it. In a browser, go to *https://huggingface.co/models* and search for the model name 'Helsinki-NLP/opus-mt-en-fr' (or you can just go to huggingface.co/Helsinki-NLP/opus-mt-en-fr).
![model search](./images/gaidd26.png?raw=true "model search")

3. You can look around on the model card for more info about the model. Notice that it has links to an *OPUS readme* and also links to download its original weights, translation test sets, etc.

4. When done looking around, go back to the repository and look at the rest of the *translation.py* file. What we are doing is loading the model, the tokenizer, and then taking a set of random texts and running them through the tokenizer and model to do the translation. Go ahead and execute the code in the terminal via the command below.
```
python translation.py
```
![translation by model](./images/gaidd41.png?raw=true "translation by model")
 
5. There's also an example program for doing classification. The file name is classification.py. Open the file either by clicking on [**genai/classification.py**](./genai/classification.py) or by entering the command below in the codespace's terminal.

```
code classification.py
```
6. Take a look at the model for this one *joeddav/xlm-roberta-large-xnli* on huggingface.co and read about it. When done, come back to the repo.

7. *classification.py* uses a HuggingFace pipeline to do the main work. Notice it also includes a list of categories as *candidate_labels* that it will use to try and classify the data. Go ahead and run it to see it in action. (This will take awhile to download the model.) After it runs, you will see each topic, followed by the ratings for each category. The scores reflect how well the model thinks the topic fits a category. The highest score reflects which category the model thinks fit best.
```
python classification.py
```
![classification by model](./images/gaidd42.png?raw=true "classification by model")

8. Finally, we have a program to do sentiment analysis. The file name is sentiment.py. Open the file either by clicking on [**genai/sentiment.py**](./genai/sentiment.py) or by entering the command below in the codespace's terminal.

```
code sentiment.py
```

9. Again, you can look at the model used by this one *distilbert-base-uncased-finetuned-sst-2-english* in Hugging Face.

10. When ready, go ahead and run this one in the similar way and observe which ones it classified as positive and which as negative and the relative scores.
```
python sentiment.py
```
![sentiment by model](./images/gaidd43.png?raw=true "sentiment by model")

11. If you're done early, feel free to change the texts, the candidate_labels in the previous model, etc. and rerun the models to see the results.

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 5 - Using Ollama to run models locally**

**Purpose: In this lab, we’ll start getting familiar with Ollama, a way to run models locally.**

1. We already have a script that can download and start Ollama and fetch some models we'll need in later labs. Take a look at the commands being done in the [**scripts/startOllama.sh**](./scripts/startOllama.sh) file. 
```
cat ../scripts/startOllama.sh
```

2. Go ahead and run the script to get Ollama and start it running.
```
../scripts/startOllama.sh &
```

The '&' at the end will causes the script to run in the background. You will see a set of startup messages. After those, you can just hit *Enter* to get back to a prompt in the terminal.

![starting ollama](./images/gaidd81.png?raw=true "starting ollama")

3. Now let's find a model to use. Go to https://ollama.com and in the *Search models* box at the top, enter *llama*. In the list that pops up, choose the entry for "llama3.2".

![searching for llama](./images/31ai7.png?raw=true "searching for llama")

4. This will put you on the specific page about that model. Scroll down and scan the various information available about this model.
![reading about llama3.2](./images/31ai8.png?raw=true "reading about llama3.2")

5. Switch back to a terminal in your codespace. While it's not necessary to do as a separate step, first pull the model down with ollama. (This will take a few minutes.)

```
ollama pull llama3.2
```
![pulling the model](./images/gaidd96.png?raw=true "pulling the model")

6. Once the model is downloaded, run it with the command below.
```
ollama run llama3.2
```

7. Once this starts, you can query the model by inputting text at the *>>>Send a message (/? for help)* prompt.  Let's ask it about what the weather is in Paris. What you'll see is it telling you that it doesn't have access to current weather data and suggesting some ways to gather it yourself.

```
What's the current weather in Paris?
```

![answer to weather prompt and response](./images/31ai10.png?raw=true "answer to weather prompt and response")

8. Now, let's try a call with the API. You can stop the current run with a Ctrl-D or switch to another terminal. Then put in the command below (or whatever simple prompt you want). 
```
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "What causes weather changes?",
  "stream": false
}'
```

9. This will take a minute or so to run. You should see a single response object returned with lots of data. But you can make out the text answer if you look for it. You can try out some other prompts/queries if you want.

![query response](./images/gaidd83.png?raw=true "Query response")

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 6 - Creating a simple agent**

**Purpose: In this lab, we’ll learn about the basics of agents and see how tools are called. We'll also see how Chain of Thought prompting works with LLMs and how we can have ReAct agents reason and act.**

1. For this lab, we have the outline of an agent in a file called *agent.py* in that directory. You can take a look at the code either by clicking on [**agent.py**](./agent.py) or by entering the command below in the codespace's terminal.
   
```
code agent.py
```
![starting agent code](./images/gaidd84.png?raw=true "Starting agent code")

2. As you can see, this outlines the steps the agent will go through without all the code. When you are done looking at it, close the file by clicking on the "X" in the tab at the top of the file.

3. Now, let's fill in the code. To keep things simple and avoid formatting/typing frustration, we already have the code in another file that we can merge into this one. Run the command below in the terminal.

```
code -d ../extra/lab6-agent.txt agent.py
```

4. Once you have run the command, you'll have a side-by-side in your editor of the completed code and the agent1.py file.
  You can merge each section of code into the agent1.py file by hovering over the middle bar and clicking on the arrows pointing right. Go through each section, look at the code, and then click to merge the changes in, one at a time. Make sure to merge all the changes.

![Side-by-side merge](./images/gaidd85.png?raw=true "Side-by-side merge") 

5. When you have finished merging all the sections in, the files should show no differences. Save the changes simply by clicking on the "X" in the tab name.

![Merge complete](./images/gaidd86.png?raw=true "Merge complete") 

6. Now you can run your agent with the following command:

```
python agent.py
```

7. The agent will start running and will prompt for a location (or "exit" to finish). At the prompt, you can type in a location like "Paris, France" or "London" or "Raleigh" and hit *Enter*. You'll see lots of activity while the model is loaded. After that you'll be able to see the Thought -> Action -> Observation loop in practice as each one is listed out. You'll also see the arguments being passed to the tools as they are called. Finally you should see a human-friendly message summarizing the weather forecast.

![Agent run](./images/gaidd87.png?raw=true "Agent run") 

8. You can then input another location and run the agent again or exit. Note that if you get a timeout error, the API may be limiting the number of accesses in a short period of time. You can usually just try again and it will work.

<p align="center">
**[END OF LAB]**
</p>
</br></br>


**Lab 7 - Working with Vector Databases**

**Purpose: In this lab, we’ll learn about how to use vector databases for storing supporting data and doing similarity searches.**

1. For this lab and the next one, we have a data file that we'll be usihg that contains a list of office information and details for a ficticious company. The file is in [**data/offices.pdf**](./data/offices.pdf). You can use the link to open it and take a look at it.

![PDF data file](./images/31ai23.png?raw=true "PDF data file") 

2. In our repository, we have some simple tools built around a popular vector database called Chroma. There are two files which will create a vector db (index) for the *.py files in our repo and another to do the same for the office pdf. You can look at the files either via the usual "code <filename>" method or clicking on [**tools/index_code.py**](./tools/index_code.py) or [**tools/index_pdf.py**](./tools/index_pdf.py).

```
code ../tools/index_code.py
code ../tools/index_pdf.py
```

3. Let's create a vector database of our local python files. Run the program to index those. **This may run for a while before you see things happening.** You'll see the program loading the embedding model that will turn the code chunks into numeric represenations in the vector database and then it will read and index our *.py files. It will create a new local vector database in *./chroma_db*.

```
python ../tools/index_code.py
```

![Running code indexer](./images/gaidd88.png?raw=true "Running code indexer")

4. To help us do easy/simple searches against our vector databases, we have another tool at [**tools/search.py**](./tools/search.py). This tool connects to the ChromaDB vector database we create, and, using cosine similarity metrics, finds the top "hits" (matching chunks) and prints them out. You can open it and look at the code in the usual way if you want. No changes are needed to the code.

```
code ../tools/search.py
```

5. Now, let's run the search tool against the vector database we built in step 3. You can prompt it with phrases related to our coding like any of the ones shown below. When done, just type "exit".  Notice the top hits and their respective cosine similarity values. Are they close? Farther apart?

```
python ../tools/search.py

convert celsius to farenheit
embed model sentence-transformers
```

![Running search](./images/gaidd89.png?raw=true "Running search")

6.  Now, let's recreate our vector database based off of the PDF file. Just run the indexer for the pdf file.

```
python ../tools/index_pdf.py
```

![Indexing PDF](./images/gaidd90.png?raw=true "Indexing PDF")

7. Now, we can run the same search tool to find the top hits for information about offices. Below are some prompts you can try here. Note that in some of them, we're using keywords only found in the PDF document. Notice the cosine similarity values on each - are they close? Farther apart?  When done, just type "exit".

```
python ../tools/search.py

Queries:
Corporate Operations office
Seaside cities
Tech Development sites
High revenue branch
```

![PDF search](./images/gaidd95.png?raw=true "PDF search")

8. Keep in mind that this is not trying to intelligently answer your prompts at this point. This is a simple semantic search to find related chunks. In lab 8, we'll add in the LLM to give us better responses. In preparation for that lab, make sure that indexing for the PDF is the last one you ran and not the indexing for the Python files.


<p align="center">
**[END OF LAB]**
</p>
</br></br>

    
**Lab 8 - Using RAG with Agents**

**Purpose: In this lab, we’ll explore how agents can leverage external data stores via RAG and tie in our previous tool use.**

1. For this lab, we're going to combine our previous agent that looks up weather with RAG to get information about offices based on a prompt and tell us what the weather is like for that locaion.

2. We have a starter file for the new agent with rag in [**rag-agent.py**](./rag_agent.py).As before, we'll use the "view differences and merge" technique to learn about the code we'll be working with. The command to run this time is below. There are a number of helper functions in this code that are useful to understand. Take some time to look at each section as you merge them in.
   
```
code -d ../extra/lab8-agent.txt rag_agent.py
```

![Code for rag agent](./images/gaidd91.png?raw=true "Code for rag agent") 


3. When you're done merging, close the merge tab as before. Then you can start the new agent running.

```
python rag_agent.py
```

5. You'll see a *Prompt:* prompt when it is ready for input from you. The agent is geared around you entering a prompt about an office. Try a prompt like one of the ones below about office "names" that are only in the PDF.

```
Tell me about HQ
Tell me about the Southern office
```

6. What you should see after that are some messages that show internal processing, such as the retrieved items from the RAG datastore.  Then the agent will run through the necessary steps like parsing the query to find a location, getting the coordinates for the location, getting the weather etc. At the end it will print out an answer to your prompt and the weather determined from the tool.
 
![Running the RAG agent](./images/gaidd92.png?raw=true "Running the RAG agent") 

7. After the initial run, you can try prompts about other offices or cities mentioned in the PDF. Type *exit* when done.

8. While this works, it could be more informative and user-friendly. Let's change the prompt and directions to the LLM to have it add an additional fact about the city where the office is located and include that and the weather in a more user-friendly response. To see and make the changes you can do the usual diff and merge using the command below.

```
code -d ../extra/lab8-agent-2.txt rag_agent.py
```

![Updating the RAG agent](./images/gaidd93.png?raw=true "Updating the RAG agent") 

9. Once you've finished the merge, you can run the new agent code the same way again.

```
python rag_agent.py
```

10. Now, you can try the same queries as before and you should get more user-friendly answers, based off primarily our changes to the prompt.

```
Tell me about HQ
Tell me about the Southern office
```

![Running the updated RAG agent](./images/gaidd94.png?raw=true "Running the updated RAG agent") 

<p align="center">
**[END OF LAB]**
</p>
</br></br>

</br></br>
<p align="center">
**THANKS!**
</p>
